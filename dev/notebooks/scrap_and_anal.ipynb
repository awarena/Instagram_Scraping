{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === User-Agent Rotation ===\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing metadata...\n",
      "An error occurred: 'date_posted'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alex Hoang\\AppData\\Local\\Temp\\ipykernel_9616\\597891650.py\", line 182, in <module>\n",
      "    metadata_list = scrape_metadata(driver, post_urls)\n",
      "  File \"C:\\Users\\Alex Hoang\\AppData\\Local\\Temp\\ipykernel_9616\\597891650.py\", line 125, in scrape_metadata\n",
      "    data = [metadata for metadata in data if metadata['date_posted']]\n",
      "  File \"C:\\Users\\Alex Hoang\\AppData\\Local\\Temp\\ipykernel_9616\\597891650.py\", line 125, in <listcomp>\n",
      "    data = [metadata for metadata in data if metadata['date_posted']]\n",
      "KeyError: 'date_posted'\n"
     ]
    }
   ],
   "source": [
    "def get_random_user_agent():\n",
    "    ua = UserAgent()\n",
    "    return ua.random\n",
    "\n",
    "def load_cookies_from_file(driver, path):\n",
    "    with open(path, 'r') as cookiesfile:\n",
    "        cookies = json.load(cookiesfile)\n",
    "        for cookie in cookies:\n",
    "            if 'domain' in cookie and cookie['domain'].startswith('.'):\n",
    "                cookie['domain'] = cookie['domain'][1:]\n",
    "            if 'sameSite' in cookie and cookie['sameSite'] not in [\"Strict\", \"Lax\", \"None\"]:\n",
    "                del cookie['sameSite']\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "def save_cookies_to_file(driver, path):\n",
    "    with open(path, 'w') as cookiesfile:\n",
    "        json.dump(driver.get_cookies(), cookiesfile)\n",
    "\n",
    "def simulate_user_interaction(driver):\n",
    "    \"\"\"Simulate human-like mouse movements and scrolling.\"\"\"\n",
    "    action = ActionChains(driver)\n",
    "    action.move_by_offset(random.randint(1, 300), random.randint(1, 300)).perform()  # Random mouse movement\n",
    "    driver.execute_script(\"window.scrollBy(0, 300);\")  # Scroll slightly\n",
    "    time.sleep(random.uniform(1, 3))  # Random delay\n",
    "\n",
    "def collect_hashtag_posts(driver, hashtag, num_posts=10):\n",
    "    driver.get(f\"https://www.instagram.com/explore/search/keyword/?q={hashtag}\")\n",
    "    time.sleep(15)\n",
    "    print(\"Page loaded\")\n",
    "\n",
    "    unique_posts = set()\n",
    "    count = 0\n",
    "    while len(unique_posts) < num_posts:\n",
    "        current_posts = len(unique_posts)\n",
    "\n",
    "        for i in range(3):\n",
    "            # simulate_user_interaction(driver)  # Simulate user behavior during scrolling\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(2, 5))  # Random delay after each scroll\n",
    "            print(f\"Scrolled {i+1} times\")\n",
    "\n",
    "        post_links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        print(f\"Found {len(post_links)} links\")\n",
    "\n",
    "        post_links = list(set(post_links))  # Remove duplicates\n",
    "\n",
    "        for link in post_links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and '/p/' in href:\n",
    "                if len(unique_posts) >= num_posts:\n",
    "                    break\n",
    "                unique_posts.add(href)\n",
    "\n",
    "        if len(unique_posts) == current_posts:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "        print(f\"Found {len(unique_posts)} unique posts\")\n",
    "\n",
    "    unique_posts_json = json.dumps(list(unique_posts), indent=4)\n",
    "    with open(f'instagram_posts_urls_{len(unique_posts)}.json', 'w') as json_file:\n",
    "        json_file.write(unique_posts_json)\n",
    "    print(\"Count:\", count)\n",
    "    return unique_posts\n",
    "\n",
    "def get_proxies():\n",
    "    response = requests.get('https://api.proxyscrape.com/v4/free-proxy-list/get?request=display_proxies&protocol=socks4&proxy_format=protocolipport&format=text&timeout=20000')\n",
    "    proxies = response.text.strip().split('\\n')\n",
    "    return proxies\n",
    "\n",
    "def set_proxy(options, proxy):\n",
    "    proxy = proxy.replace('socks4://', '')\n",
    "    options.add_argument(f'--proxy-server=socks4://{proxy}')\n",
    "\n",
    "def extract_post_metadata(driver, post_url):\n",
    "    driver.get(post_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    metadata = {}\n",
    "    try:\n",
    "        # Check for restricted content warning\n",
    "        try:\n",
    "            restricted_warning = driver.find_element(By.XPATH, \"//span[contains(text(), 'Restricted profile')]\")\n",
    "            if restricted_warning:\n",
    "                print(f\"Restricted content warning found for {post_url}. Skipping...\")\n",
    "                return None\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Check for \"page isn't available\" message\n",
    "        try:\n",
    "            page_not_available = driver.find_element(By.XPATH, \"//span[contains(text(), \\\"Sorry, this page isn't available.\\\")]\")\n",
    "            if page_not_available:\n",
    "                print(f\"Page not available message found for {post_url}. Skipping...\")\n",
    "                return None\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Try to extract the date posted\n",
    "        date_element = driver.find_element(By.XPATH, \"//time[@class='x1p4m5qa']\")\n",
    "        metadata['date_posted'] = date_element.get_attribute('datetime')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting metadata from {post_url}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def scrape_metadata(driver, post_urls, output_file='instagram_posts_metadata.json'):\n",
    "    # Load existing data from the JSON file\n",
    "    try:\n",
    "        with open(output_file, 'r') as json_file:\n",
    "            print(\"Loading existing metadata...\")\n",
    "            data = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "            print(\"No existing metadata found\")\n",
    "            data = []\n",
    "\n",
    "    if data:\n",
    "        # Remove metadata entries without a 'date_posted' field\n",
    "        data = [metadata for metadata in data if metadata['date_posted']]\n",
    "    \n",
    "    for url in post_urls:\n",
    "        if any(metadata['url'] == url for metadata in data):\n",
    "            print(f\"Metadata for {url} already exists. Skipping...\")\n",
    "            continue\n",
    "        if len(data) % 10 == 0 and len(data) > 0:\n",
    "            wait_time = random.randint(30, 300)\n",
    "            print(f\"Pausing for {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        metadata = extract_post_metadata(driver, url)\n",
    "\n",
    "        if metadata is None:  # If metadata is None, skip this post\n",
    "            continue\n",
    "\n",
    "        if not metadata:  # If metadata is empty, wait and retry randomly from 1 to 3 hours\n",
    "            wait_time = random.randint(3600, 10800)\n",
    "            print(f\"No metadata found for {url}. Waiting {wait_time} seconds before retrying...\")\n",
    "            time.sleep(wait_time)\n",
    "            metadata = extract_post_metadata(driver, url)  # Retry after an hour\n",
    "\n",
    "        metadata['url'] = url\n",
    "        data.append(metadata)\n",
    "\n",
    "        # Save metadata to file after each post\n",
    "        with open(output_file, 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    HASHTAG = 'lashartist'\n",
    "    service = Service(r'E:\\Tools\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')\n",
    "\n",
    "    # Fetch proxies\n",
    "    proxies = get_proxies()\n",
    "    proxy_index = 0\n",
    "\n",
    "    # Chrome options with random User-Agent\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    options.add_argument(f\"user-agent={get_random_user_agent()}\")  # Rotate User-Agent\n",
    "    options.binary_location = r'E:\\Tools\\chrome-win64\\chrome-win64\\chrome.exe'\n",
    "\n",
    "    # Session 1: Scrape post URLs with loaded cookies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Session 2: Scrape post metadata without loading cookies\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        with open('instagram_posts_urls_10000.json', 'r') as json_file:\n",
    "            post_urls = json.load(json_file)\n",
    "        metadata_list = scrape_metadata(driver, post_urls)\n",
    "\n",
    "        metadata_json = json.dumps(metadata_list, indent=4)\n",
    "        print(metadata_json)\n",
    "        \n",
    "        with open('instagram_posts_metadata.json', 'w') as json_file:\n",
    "            json_file.write(metadata_json)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # Session 3: Check if there are empty metadata entries and retry, if necessary\n",
    "\n",
    "    # driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # try:\n",
    "    #     with open('instagram_posts_metadata.json', 'r') as json_file:\n",
    "    #         metadata_list = json.load(json_file)\n",
    "\n",
    "    #     # Identify empty metadata entries based on the 'date_posted' field\n",
    "    #     empty_metadata = [metadata for metadata in metadata_list if not metadata['date_posted']]\n",
    "    #     print(f\"Found {len(empty_metadata)} empty metadata entries\")\n",
    "\n",
    "    #     if empty_metadata:\n",
    "    #         post_urls = [metadata['url'] for metadata in empty_metadata]\n",
    "    #         retried_metadata_list = scrape_metadata(driver, post_urls)\n",
    "\n",
    "    #         # Remove empty metadata entries from the original list\n",
    "    #         # metadata_list = [metadata for metadata in metadata_list if metadata['date_posted']]\n",
    "\n",
    "    #         # Add retried metadata to the list\n",
    "    #         # metadata_list.extend(retried_metadata_list)\n",
    "\n",
    "    #         # metadata_json = json.dumps(metadata_list, indent=4)\n",
    "    #         # print(metadata_json)\n",
    "\n",
    "    #         # with open('instagram_posts_metadata.json', 'w') as json_file:\n",
    "    #         #     json_file.write(metadata_json)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred: {e}\")\n",
    "    #     traceback.print_exc()\n",
    "\n",
    "    # finally:\n",
    "    #     driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap_anal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
